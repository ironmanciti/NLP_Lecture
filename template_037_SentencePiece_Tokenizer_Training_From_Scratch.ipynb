{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3754afcd",
   "metadata": {
    "id": "whjogPl1KL4-"
   },
   "source": [
    "# 037. 토크나이저 처음부터 학습 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy(한국어 형태소 분석기 패키지) 설치\n",
    "# SentencePiece(서브워드 토크나이저 도구)를 최신 버전으로 업그레이드 및 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b060526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc6e1238",
   "metadata": {
    "id": "On8u7bpSjYci"
   },
   "source": [
    "# 1. Keras 기본 Tokenizer - rule-based\n",
    "- 공백 또는 구둣점으로 분리  \n",
    "- 영어 단어별로 띄어쓰기가 철저히 지켜지는 언어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b774fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 100개의 단어로 구성된 Tokenizer 객체 생성 (OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 주어진 문장 리스트에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 구축된 단어 인덱스 사전 가져오기\n",
    "# 단어 인덱스 사전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcd65f",
   "metadata": {
    "id": "0TJA8onMkTiy"
   },
   "source": [
    "Keras의 rule base tokenizer로 한글을 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e511db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 100개의 단어로 구성된 Tokenizer 객체 생성 (OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 주어진 한글 문장 리스트에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 구축된 단어 인덱스 사전 가져오기\n",
    "# 단어 인덱스 사전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7aa124",
   "metadata": {
    "id": "TkQ9u94VjYcl"
   },
   "source": [
    "# 2. 단어 사전 기반 한국어 tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74760b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt 형태소 분석기 객체 생성\n",
    "# 형태소 분석 결과를 저장할 리스트 초기화\n",
    "# 주어진 한글 문장 리스트의 각 문장에 대해 반복\n",
    "    # 문장을 형태소 분석하여 결과를 리스트에 추가\n",
    "    # 형태소 분석 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f6098",
   "metadata": {
    "id": "MKy3rIq0kuLo"
   },
   "source": [
    "사전 기반 tokenize 후 Keras tokenizer 로 vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 상위 100개의 단어로 구성된 Tokenizer 객체 생성 (OOV(Out-Of-Vocabulary) 토큰 설정)\n",
    "# 형태소 분석된 문장 리스트에 대해 토크나이저 학습 수행 (단어 인덱스 구축)\n",
    "# 구축된 단어 인덱스 사전 가져오기\n",
    "# 단어 인덱스 사전 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f063d",
   "metadata": {
    "id": "6FRpIxColOLv"
   },
   "source": [
    "두 vocabulary 의 차이 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2bd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0547a3b5",
   "metadata": {
    "id": "MRtVftAuMmJ8"
   },
   "source": [
    "### 단, Okt 사전에 미등록된 단어의 경우 정확한 tokenizing 이 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 문장을 형태소 분석하여 품사 태깅 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7475b87",
   "metadata": {
    "id": "6Cq9CJhJMmJ9"
   },
   "source": [
    "예를 들어 `너무너무너무`와 `나카무라세이코`는 하나의 단어이지만, okt 사전에 등록되어 있지 않아 여러 개의 복합단어로 나뉘어집니다. 이러한 문제를 해결하기 위하여 형태소 분석기와 품사 판별기들은 사용자 사전 추가 기능을 제공합니다. 사용자 사전을 추가하여 모델의 vocabulary 를 풍부하게 만드는 것은 사용자의 몫입니다.\n",
    "\n",
    "1. okt 공식 문서를 참고해서 사용사 사전을 추가.\n",
    "2. okt를 패키징하고, konlpy에서 사용할 수 있도록 konlpy/java 경로에 jar 파일을 복사.\n",
    "3. 기존에 참고하고 있던 okt.jar 대신 새로운 okt.jar를 사용하도록 설정.\n",
    "4. konlpy 소스 경로를 import 해서 형태소 분석."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc213a",
   "metadata": {
    "id": "9NSFEmKRMmKB"
   },
   "source": [
    "# 3. Google SentencePiece Tokenizer\n",
    "\n",
    "- NAVER Movie rating data 를 이용한 sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088fa64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c19e1bda",
   "metadata": {
    "id": "Vn-WA_c6MmKC"
   },
   "source": [
    "- pandas.read_csv에서 quoting = 3으로 설정해주면 인용구(따옴표)를 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a895f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca8aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabd7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29387996",
   "metadata": {
    "id": "4h4yXblIMmKE"
   },
   "source": [
    "## 학습을 위해 text 를 따로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'nsmc.txt' 파일을 쓰기 모드로 열기 (UTF-8 인코딩 사용)\n",
    "    # 훈련 데이터의 'document' 열에 있는 각 문장에 대해 반복\n",
    "            # 문장을 파일에 쓰고 새로운 줄 추가\n",
    "            # 쓰기 오류 발생 시 오류 메시지와 해당 문장 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write 가 잘 되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 파일 경로 설정\n",
    "# 어휘 사전의 최대 크기 설정\n",
    "# 모델 파일의 접두사 설정\n",
    "# 명령어 템플릿 정의\n",
    "# 템플릿에 변수 값을 포맷하여 명령어 문자열 생성\n",
    "# 생성된 명령어 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378fefb",
   "metadata": {
    "id": "0IcsoxuvMmKF"
   },
   "source": [
    "### sentencepiece tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePieceTrainer를 사용하여 SentencePiece 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ce8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentencePieceProcessor 객체 생성\n",
    "# 학습된 SentencePiece 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터의 'document' 열에 있는 첫 세 개의 문장에 대해 반복\n",
    "    # 원본 문장 출력\n",
    "    # 문장을 SentencePiece 모델을 사용하여 토큰화하여 출력\n",
    "    # 문장을 SentencePiece 모델을 사용하여 인덱스 시퀀스로 변환하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30266d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 문장 리스트(sentences_K)에 있는 각 문장에 대해 반복\n",
    "    # 문장을 SentencePiece 모델을 사용하여 토큰화\n",
    "    # 문장을 SentencePiece 모델을 사용하여 인덱스 시퀀스로 변환\n",
    "    # 원본 문장 출력\n",
    "    # 토큰화된 결과 출력\n",
    "    # 인덱스 시퀀스 출력\n",
    "    # 각 문장 사이에 줄 바꿈 추가"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
